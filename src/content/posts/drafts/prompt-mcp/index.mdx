---
title: prompt-mcp
description: prompt-mcp
pubDate: 2025-11-25
cover: assets/cover.png
tags: ['AI', 'Agent', 'LLM', 'Function Calling', 'Prompt', 'MCP']
recommend: true
draft: true
---

### 总览

### 1.🌰 栗子first

> 不用害怕看不懂，后面会逐步讲解

![示例图片](assets/ps.png)
**逐步对应拆解：**

1. 我听说女朋友肚子疼，于是问AI agent或者说MCP client, 我女朋友肚子疼应该怎么办？
2. Agent把问题包装在user prompt中，然后agent通过MCP协议从MCP server里面获取所有tool的信息
3. Agent把这些tool的信息转化成system prompt或者转化成function calling的格式，然后和用户请求user prompt一起打包发送给AI模型。
4. AI模型发现有一个叫做web_browse的网页浏览工具可以用，于是通过普通回复或者function Calling格式产生调用这个tool的请求，希望去网上搜索答案。
5. Agent收到了这个请求之后，通过MCP协议去调用MCP server里的web browse工具。Web browse访问指定的网站并将内容返还给agent
6. agent再转发给AI模型
7. AI模型根据网页内容和自己的头脑风暴生成最终的答案---多喝热水，再返还给agent。
8. 最后由agent把结果展示给用户

### 2.🎶 从prompt说起

prompt一般来说prompt分为system prompt和user prompt

- system Prompt：系统预设的，用来设定AI模型的角色、性格、行为边界、规则
  > 用户不能随便更改system prompt，但网站也会提供一些设置，比如gpt里面有一个叫做customize chatgpt的功能，
  > 用户可以在里面写下自己的偏好，这些偏好就会变成system prompt的一部分。
- user Prompt：我们与大模型的聊天内容

不过说到底LLM还只是一个聊天机器人，在回答完问题后具体的操作还是得由你来完成，那么有没有办法让AI自己去完成任务呢？

### 3.🔥 爆火的开源Agent项目-AutoGPT

![AutoGPT](assets/autogpt.png)
它让人看到AI可以连续执行任务，从思考到行动自动循环

**AutoGPT的工作机制**

他本质是一个本地程序，想让他来操作电脑完成任务则必须：

1. 编写一些函数（Tools）

- `get_install_url()`
- `search_web()`
- `install_software()`

2. 给每一个函数写一个说明（自然语言）

- 功能是什么
- 参数怎么写
- 返回什么格式

3. 把这些函数描述注册给AutoGPT
4. AutoGPT会把这些信息写进system Prompt
   例如：如果你想调用XXX工具，请返回-我要调用 + 工具名 + 参数

然后AutoGPT将system prompt和user prompt一起发给GPT模型

**我们来看一个例子**

1. 我输入：
   > 帮我找一下 CS 的安装地址
2. Agent(AutoGPT)将任务与可用工具写入system prompt交给AI模型，可能包含：
   ```javascript
   你可以使用函数 get_install_url(name)
   它用于查询软件的下载/安装地址
   返回格式必须为 JSON { "Tool": "get_install_url", "args": {...}}
   ```
3. AI模型 阅读prompt并推断需要实用工具，根据说明返回
   ```json
   { "Tool": "get_install_url", "args": { "name": "CS" } }
   ```
4. AutoGPT 解析 AI模型的输出，调用`get_install_url("CS")`
5. 工具返回结果给Agent，再由Agent继续发给 AI模型，可能返回
   ```json
   { "url": "D:/games/counterstrike" }
   ```
6. AI模型根据返回的结果继续resoning或给出最终回答
   > 你可以在 D:/games/counterstrike 找到 CS

人们把AutoGPT这种负责在模型、工具和最终用户之间传话的程序就叫做AI Agent，而那些提供给AI调用的函数或服务就叫做Agent Tools。

虽然 AutoGPT 强，但它依赖的是：AI 能否乖乖按照自然语言规则格式化输出。而 LLM 本质是概率模型，因此会出现忘记格式、缺少字段、JSON不合法等问题

于是 Agent 必须写很多“重试逻辑”：检查格式是否正确，不对就重新发给模型再试再试再试...像 Cline 等 Agent 至今仍使用“多轮重试”的策略。

于是我们就需要一种更可靠的机制来确保 AI 按照我们的要求进行格式化输出。

tips:

> 现在，随着下面即将要讲的 Function Calling 的广泛应用，上面反复错误后重试方式正在被取代。现代大模型在结构化调用时，并不是等它出错再重试，
> 而是利用 约束解码（constrained decoding）：在每一步生成 token 时，模型只被允许从「那些符合预定义 schema／合法结构」的 token 中选择。这样，非法 token 根本不会出现在它的备选里，也就避免了格式错误。
> 当然，重试机制并没有完全消失，它仍然可能用于处理 API 层面的网络抖动、限流、长上下文截断等问题。但格式层面的错误——现在已经更多由解码约束在生成时就拦截，而不是事后补救。

### 4.🔨 Function Calling：标准化的工具调用方式

> Function Calling 工作流程示意图如下所示，来源链接🔗https://help.aliyun.com/zh/model-studio/qwen-function-calling

![functioncalling](assets/functioncalling.png)

为了解决刚才提到的“不稳定的输出格式”，模型厂商开始做统一标准，它不再用自然语言描述工具，而是使用**JSON Schema + 标准调用格式**

回到之前找CS安装地址的问题，我们通过system prompt告诉AI有哪些工具以及返回格式，但是这些描述都是用自然语言随意写的
Function Calling则对这些描述进行了标准化，将工具描述和返回格式都用JSON描述，这样Agent就可以根据JSON的格式进行解析，从而实现功能调用。
例如：

```json
"tools": [
    {
      "name": "get_install_url",
      "description": "Get the installation URL of a software",
      "parameters": {
        "type": "object",
        "properties": {
          "software": {
            "type": "string",
            "description": "The name of the software"
          }
        },
        "required": ["software"]
      }
    }
]
```

然后AI使用工具时的回复也都依照相同的格式

```json
"tool_calls": [
    {
      "id": "call_123",
      "type": "function",
      "function": {
        "name": "get_install_url",
        "arguments": "{\"software\":\"CS\"}"
      }
    }
]
```

于是人们就能更加有针对性的训练AI模型，让他理解这种调用的场景以及什么是funcition calling和按照什么格式调用工具
在这种情况下如果AI生成了错误的回复，因为回复的格式是规定的，AI服务器端自己就可以检测到并且进行重试，用户根本感觉不到
不仅降低了用户端的开发难度，也节省了用户端重试带来的Token开销，正是由这些好处，现在越来越多的AI Agent开始从system Prompt转向Function Calling

**当然Function Calling仍然不是完美的**

虽然各大厂都支持 function calling，但：

- 每家厂商的格式略有差异
- 早期开源模型不支持

想写一个“跨模型通用 Agent”依然很麻烦

因此，目前市面上：system prompt + function calling 并存。

而且这只是 AI模型 ↔ Agent 之间的通信方式。接下来我们来讲 Agent ↔ 工具（Tool）之间的通信。

### 5.🤖 Tools如何提供给Agent？MCP给出答案

![MCP](assets/mcp.png)

**Tools如何提供给Agent呢？**

最简单的做法是把AI Agent和Agent Tools写在同一个程序里直接内部函数调用搞定，这也是大多数Agent的做法，但是后来人们逐渐发现，有些Tool的功能其实挺通用的，可以解耦出来把Tool变成服务统一托管，让所有的Agent都来调用，这就是MCP（Model Context Protocol），MCP是一个通信协议专门用来规范Agent和Tools服务之间是怎么交互的

运行Tool的服务叫做MCP server调用它的Agent叫做MCP Client，MCP规定了MCP server如何和MCP Client通信，以及MCP Server要提供哪些接口，比如说用来查询MCP server中有哪些Tool、Tool的功能、描述需要的参数、格式等等的接口

除了普通的Tool这种函数调用的形式MCP Server也可以直接提供数据提供类似文件读写的服务叫做Resource，或者为Agent提供提示词的模板叫做prompt，MCP Server既可以和Agent跑在同一台机器上通过标准输入输出进行通信，也可以被部署在网络上通过HTTP进行通信

这里需要注意的是虽然MCP是为了AI而定制出来的标准但实际上MCP本身却和AI模型没有关系，他并不关心Agent用的是哪个模型，MCP只负责帮Agent管理工具、资源和提示词MCP的目标是统一Agent与外部世界的能力接口，让AI能访问“操作系统级能力”

**MCP的优势**

- 工具复用：一个工具可被任何 Agent 使用
- 统一格式：所有工具都是同一种描述方式
- 解耦：工具与 Agent 彻底分离
- 跨平台：可以用在本地、云端、VSCode、浏览器等环境
- 模型无关：不绑定 GPT、Claude、Llama

这让 Agent 的生态有了“插件系统”的可能。

### 6.🔙 回看样例

![示例图片](assets/ps.png)
综上就是system prompt、user prompt、AI agent、agent to function calling、MCP、AI模型之间的联系与区别了

他们不是彼此取代的关系，而是像齿轮一样，一起构成了AI自动化协作的完整体系

**再举一个🌰**

> 用户（客户） → Agent（项目经理） → 模型（顾问） → MCP Server（外包工具团队）

1. 用户把需求告诉项目经理（Agent）。
2. 项目经理不会自己想答案，于是把需求转给顾问（AI 模型）。
3. 同时，项目经理还会把手上所有“可用外包团队”（MCP Server 提供的工具列表）发给顾问看。
   早期，项目经理只能用自然语言解释这些团队的服务范围（system prompt），说不清楚时顾问就会误解。
   后来大家统一用一份结构化的“外包团队服务手册”（Function Calling / JSON Schema），顾问就不会理解错误。
4. 顾问分析后告诉项目经理：“要完成需求，需要叫这个外包团队（某个 tool）来做”。
5. 项目经理自己去调用外包团队的接口（MCP Server），拿到结果后再给顾问复核。
6. 顾问确认结果满足需求后，项目经理把最终成果交给用户。

在 MCP 语境下：

- 项目经理 = MCP Client
- 外包团队 = MCP Server

### 7.❓ 我的疑问总结

1. 问：如果有大量 MCP 服务节点，把所有工具的 schema 放进 system prompt，不会超过 LLM 的上下文极限吗？

   答：不会。现代 Agent 系统不存在“把所有工具一次性塞给模型”的做法。工程上有三层优化策略。

   1. 工具是“按需注入”（On-Demand Injection）
      Agent 会根据任务意图筛选工具，例如：
      用户提到“查文件”，只加载 file 系工具
      用户提到“浏览网页”，只加载 browse 工具
      常见的方法包括意图识别（Intent Classification）与工具类型匹配。
      最终模型只看到与当前任务相关的少量工具，而非全部服务节点。
   2. 工具通过“向量检索”选择（Vector-Based Tool Retrieval）
      每个工具的描述会被向量化
      当用户提出需求时，Agent 检索出 Top-K（通常 3〜10 个）最相关的工具注入模型。
      这种做法让工具数量从几十到几万都不影响 prompt 大小。
   3. 工具 Schema 不一定放在 prompt，而是通过模型 API 的“结构化字段”传入
      在 OpenAI / Anthropic / Google 的 function/Tool calling 中：
      工具 schema 是一个独立字段
      不占用上下文 token
      不以系统提示词形式写入 prompt
      模型内部使用结构化约束解析调用，不影响上下文容量。
   4. 📌 总结（适合写成一行）
      工具不是“塞进 prompt”，而是“筛选后以结构化方式注入”。工具越多，对上下文压力越小，因为只有相关工具会进入模型。

2. 问：为什么和我理解的Agent又好像不一样了呢：

   答：广义的 Agent 指整个系统级智能体，包括大模型、工具/MCP 服务、记忆和工作流等，能够感知、规划、决策并执行任务。
   狭义的 Agent 则通常特指负责任务规划和工具选择的模块，核心由 LLM 提供推理和决策能力。
   换句话说，广义是“系统整体”，狭义是“决策调度组件”，两者层级不同，但都以 LLM 为核心智能来源。

3. 问：整体上怎么理解 MCP 工具与大模型的关系？
   答：MCP 工具的选择和调用依赖大模型的推理能力，但现代系统会结合意图识别和代理层，兼顾工具丰富性和上下文效率。大模型负责理解和决策，Agent/中间层负责工具调用和结果管理。

4. 问：MCP 是不是直接给 LLM 使用的协议？LLM 是否可以直接使用 MCP 来调用工具？
   答：不是。MCP 是 Agent 与工具之间的标准通信协议，负责协调工具调用和数据流。LLM 本身无法主动发起交互，它只负责生成文本，实际调用和执行由 Agent 负责。MCP 是 Agent 的接口，不是 LLM 的接口。

5. 问：Function Calling (函数调用) 与 MCP 是什么关系？谁取代了谁？
   答：它们是不同层面上的互补关系，不存在取代。
   - Function Calling： 是 LLM 内部的输出机制。它是一种让 LLM 能够生成结构化 JSON 来表达调用意图的能力。
   - MCP： 是 LLM 外部的通信标准。它定义了工具如何向 Agent 暴露自身能力，以及 Agent 如何与工具进行数据流传输的规范。
   - 协同方式： Agent 会将 MCP 工具的标准化定义转换为 LLM 可以理解的 Function Calling Schema 喂给 LLM。LLM 利用 Function Calling 发出指令，Agent 利用 MCP 执行指令。

我个人的疑问：

1. Agent里面是不是还有一个LLM？LLM不应该是Agent的大脑吗？Agent可以不包含LLM是么？在图片里面LLM和Agent是分开的吗？Agent到底是不是LLM再加上一些东西的集合呢，还是说在图片里Agent也包含着一个LLM，不过旁边也有一个LLM？

这是一个非常好的问题，也是很多刚接触 **AI Agent（智能体）** 概念的人最容易混淆的地方。这张图其实画得非常清晰，它从**软件架构**和**数据流向**的角度展示了一个基于 MCP（Model Context Protocol）的 Agent 是如何工作的。

下面我结合这张图，逐一拆解你的问题。

### 1. 基于图片的分析：LLM 和 Agent 的关系

在这张图中，作者为了把“流程”讲清楚，特意把**逻辑控制部分**和**思考推理部分**拆开了：

- **中间的章鱼 (AI Agent / MCP Client)：** 这是 Agent 的**躯干和四肢**。它负责“跑腿”和“协调”。它本身没有智慧，它的作用是接收用户的指令，把指令传给大脑（LLM），然后根据大脑的指示去调用工具（MCP Server），最后把结果再搬运回给大脑。
- **右边的大脑 (AI Model)：** 这是 LLM（大语言模型）。它是 Agent 的**大脑**。它负责“思考”。它接收文本，进行推理，决定是直接回答用户，还是需要去查资料（调用工具）。
- **左边的盒子 (MCP Server)：** 这是**工具箱**。Agent 的能力边界取决于这里面有什么工具（比如图中的 `web_browse` 上网工具）。

**图解流程总结：**
用户提问 $\rightarrow$ 章鱼（Agent）把问题丢给大脑（LLM） $\rightarrow$ 大脑判断需要“上网” $\rightarrow$ 章鱼去工具箱（Server）执行上网 $\rightarrow$ 章鱼把网页内容拿回来给大脑 $\rightarrow$ 大脑总结出“多喝热水” $\rightarrow$ 章鱼告诉用户。

---

### 2. 回答你的核心疑问

#### Q1：Agent 里面是不是还有一个 LLM？

**概念上：是的。架构上：通常是分开部署的。**
在**概念**层面，我们说“这个 Agent 很聪明”，是因为它包含了一个 LLM。
但在**工程/代码**层面（就像这张图），Agent 往往是一段**控制代码**（Python/Java等写成的程序），它通过 API 去调用 LLM。

- **图中的情况：** 章鱼代表控制代码，右边的大脑代表 LLM 的 API 服务（比如 OpenAI、Claude 的接口）。它们物理上是分开的，但功能上共同组成了“Agent 系统”。

#### Q2：LLM 不应该是 Agent 的大脑吗？

**完全正确，LLM 就是大脑。**
正因为它是大脑，所以图里才把它画成一个独立的处理单元。
你可以把 Agent 想象成一个**“驾驶员（LLM）+ 汽车（Agent 框架）”**的组合：

- **LLM** 是坐在驾驶座上的人，他负责看路、做决定（左转还是右转）。
- **Agent 框架** 是这辆车，它负责执行（转动轮子）、提供感知（倒车雷达/工具）。
  这张图展示的是“车”和“人”的交互过程。

#### Q3：Agent 可以不包含 LLM 是么？

**历史定义上可以，但现在的 AI Agent 语境下通常不行。**

- **广义的 Agent（传统计算机科学）：** 只要能感知环境并采取行动的程序都叫 Agent。比如玩《超级马里奥》的自动脚本，或者扫地机器人，它们没有 LLM，也是 Agent，靠的是写死的规则或简单的强化学习网络。
- **狭义的 AI Agent（现在的热门概念）：** 特指 **LLM-based Agent**。在这种定义下，LLM 是核心组件，没有 LLM 就只是一个普通的自动化脚本，不能被称为现代意义上的 AI Agent。

#### Q4：在图片里面 LLM 和 Agent 是分开的吗？还是说这个 AI 模型其实也可以在 AI Agent 里面？

在这张图中，它们是**逻辑上分开**的组件：

- **分开的原因：** 现在的 LLM 很大（比如 GPT-4），通常运行在云端。而 Agent 的控制逻辑可能运行在你的电脑上（本地）。Agent 通过网络（API）去问 LLM 该怎么做。
- **也可以在里面吗？** 可以。如果你运行的是本地小模型（比如 Llama 3 8B），那么 LLM 和 Agent 的控制代码可能跑在同一台机器的同一个进程里。但即便如此，在代码逻辑上，它们通常还是分层的：一层负责调度（Agent），一层负责推理（LLM）。

#### Q5：Agent 到底是不是 LLM 再加上一些东西的集合呢？

**是的，这是最准确的理解。**
在这个领域有一个著名的公式（由 OpenAI 的 Lilian Weng 总结）：

$$Agent = LLM（大脑） + Planning（规划） + Memory（记忆） + Tools（工具使用）$$

回到你的图片，这正是公式的体现：

1.  **LLM：** 右边的 AI 模型。
2.  **Tools：** 左边的 MCP Server（web_browse）。
3.  **Planning & Memory：** 中间的章鱼（MCP Client）负责串联这一切，它维护着对话的历史（Context），并执行 LLM 规划出的步骤。

#### Q6：图片里 Agent 也包含着一个 LLM，不过旁边也有一个 LLM？

**不是的，图里只有一个 LLM。**
中间那个“章鱼（AI Agent）”**不包含**另一个 LLM。它只是一段**程序代码**（比如用 Python 写的 LangChain 或 AutoGen 代码）。
它的工作非常机械：

1.  它把用户的字传给右边的 LLM。
2.  它解析 LLM 返回的 JSON 格式指令（比如 `{"type": "call", "name": "web_browse"}`）。
3.  它去执行这个指令。
    **所有的智慧都只来自于右边的那个“大脑”。**

### 总结

这张图实际上是在演示 **MCP (Model Context Protocol)** 这个协议是如何连接各方的。

- **真正的“Agent”其实是整个图的总和。**
- 但在技术架构图中，为了区分职责，习惯把**“负责干活的程序”**标记为 Agent/Client，把**“负责思考的模型”**标记为 Model/LLM。

**一句话解释：**
LLM 是**大脑**，Agent 是**身体和四肢**（包含大脑的接口）。图片展示的是身体（中间）如何听从大脑（右边）的指挥，并使用工具（左边）来解决问题（给女朋友找药）的过程。

2. system prompt是什么时候传给LLM的？system prompt有没有消耗token？实际每一次对话的时候都会传一遍吗？system prompt里面会具体包含什么东西？

3. 为什么现在人人都在讨论 MCP？因为所有人意识到：真正的 AI 革命，不是更聪明的模型，而是“模型能做更多事情”。而让模型“做事情”的关键就是：
   能力接入标准化 → 工具复用 → 不同模型之间共享能力。

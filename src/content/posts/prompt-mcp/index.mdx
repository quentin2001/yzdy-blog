---
title: 串讲LLM、prompt、Agent、Function Calling、MCP
description: '从实例、原理到工程架构全面解析 Agent 的感知、记忆、工具与推理能力，带你重新理解“智能体”的真正边界。'
pubDate: 2025-11-25
cover: assets/cover.png
tags: ['AI', 'Agent', 'LLM', 'MCP', 'Prompt']
recommend: true
---

### 🌰 栗子first

> 不用害怕看不懂，后面会逐步讲解

![示例图片](assets/ps.png)

1. 我听说女朋友肚子疼，于是问AI agent或者说MCP client, 我女朋友肚子疼应该怎么办？
2. Agent把问题包装在user prompt中，然后agent通过MCP协议从MCP server里面获取所有tool的信息
3. Agent把这些tool的信息转化成system prompt或者转化成function calling的格式，然后和用户请求user prompt一起打包发送给AI模型。
4. AI模型发现有一个叫做web_browse的网页浏览工具可以用，于是通过普通回复或者function Calling格式产生调用这个tool的请求，希望去网上搜索答案。
5. Agent收到了这个请求之后，通过MCP协议去调用MCP server里的web browse工具。Web browse访问指定的网站并将内容返还给agent
6. agent再转发给AI模型
7. AI模型根据网页内容和自己的头脑风暴生成最终的答案---多喝热水，再返还给agent。
8. 最后由agent把结果展示给用户

### 从prompt说起

prompt一般来说prompt分为system prompt和user prompt，
user Prompt：我们与大模型的聊天内容
system Prompt：系统预设的，用来设定AI模型的角色、性格、行为边界、规则
system prompt用户不能随便更改但通常来说网站会提供一些设置，比如gpt里面有一个叫做customize chatgpt
的功能，用户可以在里面写下自己的偏好，这些偏好就会变成system prompt的一部分。
不过说到底LLM还只是一个聊天机器人，在回答完问题后具体的操作还是得由你来完成，那么有没有办法让AI自己去完成任务呢？

### 初期爆火的开源Agent项目-AutoGPT

它让人看到AI可以连续执行任务，从思考到行动自动循环

**AutoGPT的工作机制**
他本质是一个本地程序，想让他来操作电脑完成任务则必须：

1. 编写一些函数（Tools）

- `get_install_url()`

2. 给每一个函数写一个说明（自然语言）

- 功能是什么
- 参数怎么写
- 返回什么格式

3. 把这些函数描述注册给AutoGPT
4. AutoGPT会把这些信息写进system Prompt
   例如：如果你想调用XXX工具，请返回-我要调用 + 工具名 + 参数

然后AutoGPT将system prompt和user prompt一起发给GPT模型

**我们来看一个例子**

1. 我输入：
   > “帮我找一下 CS 的安装地址”
2. AI 看 system prompt 里有哪些工具
3. AI 决定是否调用一个 tool（例如 `get_install_url`）
4. AutoGPT 解析 AI 的输出，执行对应函数（例如 `get_install_url`）
5. get_install_url 函数结果再返回给 AI
6. AI 根据结果继续生成下一步，直到任务完成

人们把AutoGPT这种负责在模型、工具和最终用户之间传话的程序就叫做AI Agent，而那些提供给AI调用的函数或服务就叫做Agent Tools。
虽然 AutoGPT 强，但它依赖的是：AI 能否乖乖按照自然语言规则格式化输出。
而 LLM 本质是概率模型，因此会出现忘记格式、缺少字段、JSON不合法等问题
于是 Agent 必须写很多“重试逻辑”：检查格式是否正确，不对就重新发给模型再试再试再试...
像 Cline 等 Agent 至今仍使用“多轮重试”的策略。

人们把 AutoGPT 这种负责在模型、工具和最终用户之间传话的程序叫做 AI Agent，而那些被 Agent 调用的函数或服务则称为 Agent Tools。
虽然 AutoGPT 本身很强，但它曾经严重依赖模型“乖乖按照自然语言规则返回格式”，这在本质上是有风险的。因为 LLM 是概率模型，它有可能忘记遵守格式、漏掉字段，甚至输出不合法的 JSON。
为了处理这种不规范输出，传统 Agent 会加入大量重试逻辑 —— 每次检查模型是否返回了合法格式，如果不对就再发一次请求，再试一次，再试……这种策略鲁棒性差，也浪费 Token。

于是我们就需要一种更可靠的机制来确保 AI 按照我们的要求进行格式化输出。

tips:

> 现在，随着 Function Calling 的广泛应用，这种方式正在被取代。现代大模型在结构化调用时，并不是等它出错再重试，
> 而是利用 约束解码（constrained decoding）：在每一步生成 token 时，模型只被允许从「那些符合预定义 schema／合法结构」的 token 中选择。这样，非法 token 根本不会出现在它的备选里，也就避免了格式错误。
> 当然，重试机制并没有完全消失，它仍然可能用于处理 API 层面的网络抖动、限流、长上下文截断等问题。但格式层面的错误——现在已经更多由解码约束在生成时就拦截，而不是事后补救。

### Function Calling：标准化的工具调用方式

为了解决“不稳定的输出格式”，模型厂商开始做统一标准，它不再用自然语言描述工具，而是使用**JSON Schema + 标准调用格式**

回到之前找CS安装地址的问题，我们通过system prompt告诉AI有哪些工具以及返回格式，但是这些描述都是用自然语言随意写的
Function Calling则对这些描述进行了标准化，将工具描述和返回格式都用JSON描述，这样Agent就可以根据JSON的格式进行解析，从而实现功能调用。
例如：

```json
"tools": [
    {
      "name": "get_install_url",
      "description": "Get the installation URL of a software",
      "parameters": {
        "type": "object",
        "properties": {
          "software": {
            "type": "string",
            "description": "The name of the software"
          }
        },
        "required": ["software"]
      }
    }
]
```

然后AI使用工具时的回复也都依照相同的格式

```json
"tool_calls": [
    {
      "id": "call_123",
      "type": "function",
      "function": {
        "name": "get_install_url",
        "arguments": "{\"software\":\"CS\"}"
      }
    }
]
```

于是人们就能更加有针对性的训练AI模型，让他理解这种调用的场景以及什么是funcition calling和按照什么格式调用工具
在这种情况下如果AI生成了错误的回复，因为回复的格式是规定的，AI服务器端自己就可以检测到并且进行重试，用户根本感觉不到
不仅降低了用户端的开发难度，也节省了用户端重试带来的Token开销，正是由这些好处，现在越来越多的AI Agent开始从system Prompt转向
Function Calling，

**当然Function Calling仍然不是完美的**
虽然各大厂都支持 function calling，但：
每家厂商的格式略有差异
早期开源模型不支持
想写一个“跨模型通用 Agent”依然很麻烦
因此，目前市面上：
system prompt + function calling 并存。
而且这只是 AI ↔ Agent 之间的通信方式。
接下来我们来讲 Agent ↔ 工具（Tool）之间的通信。

### Tools如何提供给Agent？MCP给出答案

tools如何提供给Agent呢？最简单的做法是把AI Agent和Agent Tools写在同一个程序里
直接内部函数调用搞定，这也是大多数Agent的做法，但是后来人们逐渐发现，有些Tool的功能其实挺通用的，可以解耦出来
把Tool变成服务统一托管，让所有的Agent都来调用，这就是MCP（Model Context Protocol），MCP是一个通信协议，
专门用来规范Agent和Tools服务之间是怎么交互的，
运行Tool的服务叫做MCP server调用它的Agent叫做MCP Client，MCP规定了MCP server如何和MCP Client通信，
以及MCP Server要提供哪些接口，
比如说用来查询MCP server中有哪些Tool、Tool的功能、描述需要的参数、格式等等的接口，
除了普通的Tool这种函数调用的形式MCP Server也可以直接提供数据
提供类似文件读写的服务叫做Resource，或者为Agent提供提示词的模板叫做prompt，MCP Server既可以和Agent跑在同一台机器上
通过标准输入输出进行通信，也可以被部署在网络上通过HTTP进行通信，这里需要注意的是虽然MCP是为了AI而定制出来的标准
但实际上MCP本身却和AI模型没有关系，他并不关心Agent用的是哪个模型，MCP只负责帮Agent管理工具、资源和提示词
MCP的目标是统一Agent与外部世界的能力接口，让AI能访问“操作系统级能力”

**MCP的优势**

- 工具复用：一个工具可被任何 Agent 使用
- 统一格式：所有工具都是同一种描述方式
- 解耦：工具与 Agent 彻底分离
- 跨平台：可以用在本地、云端、VSCode、浏览器等环境
- 模型无关：不绑定 GPT、Claude、Llama

这让 Agent 的生态有了“插件系统”的可能。

### 回看样例

![示例图片](assets/ps.png)
综上就是system prompt、user prompt、AI agent、agent to function calling、MCP、AI模型之间的联系与区别了，
他们不是彼此取代的关系，而是像齿轮一样，一起构成了AI自动化协作的完整体系
**再举一个🌰**
用户（客户） → Agent（项目经理） → 模型（顾问） → MCP Server（外包工具团队）
用户把需求告诉项目经理（Agent）。
项目经理不会自己想答案，于是把需求转给顾问（AI 模型）。
同时，项目经理还会把手上所有“可用外包团队”（MCP Server 提供的工具列表）发给顾问看。
早期，项目经理只能用自然语言解释这些团队的服务范围（system prompt），说不清楚时顾问就会误解。
后来大家统一用一份结构化的“外包团队服务手册”（Function Calling / JSON Schema），顾问就不会理解错误。
顾问分析后告诉项目经理：“要完成需求，需要叫这个外包团队（某个 tool）来做”。
项目经理自己去调用外包团队的接口（MCP Server），拿到结果后再给顾问复核。
顾问确认结果满足需求后，项目经理把最终成果交给用户。

在 MCP 语境下：
项目经理 = MCP Client
外包团队 = MCP Server

### ❓ 疑问总结

1. 如果有大量 MCP 服务节点，把所有工具的 schema 放进 system prompt，不会超过 LLM 的上下文极限吗？
   回答：不会。现代 Agent 系统不存在“把所有工具一次性塞给模型”的做法。工程上有三层优化策略。
   1. 工具是“按需注入”（On-Demand Injection）
      Agent 会根据任务意图筛选工具，例如：
      用户提到“查文件”，只加载 file 系工具
      用户提到“浏览网页”，只加载 browse 工具
      常见的方法包括意图识别（Intent Classification）与工具类型匹配。
      最终模型只看到与当前任务相关的少量工具，而非全部服务节点。
   2. 工具通过“向量检索”选择（Vector-Based Tool Retrieval）
      每个工具的描述会被向量化
      当用户提出需求时，Agent 检索出 Top-K（通常 3〜10 个）最相关的工具注入模型。
      这种做法让工具数量从几十到几万都不影响 prompt 大小。
   3. 工具 Schema 不一定放在 prompt，而是通过模型 API 的“结构化字段”传入
      在 OpenAI / Anthropic / Google 的 function/tool calling 中：
      工具 schema 是一个独立字段
      不占用上下文 token
      不以系统提示词形式写入 prompt
      模型内部使用结构化约束解析调用，不影响上下文容量。
      📌 总结（适合写成一行）
      工具不是“塞进 prompt”，而是“筛选后以结构化方式注入”。工具越多，对上下文压力越小，因为只有相关工具会进入模型。
2. 为什么和我理解的Agent又好像不一样了呢：
   广义的 Agent 指整个系统级智能体，包括大模型、工具/MCP 服务、记忆和工作流等，能够感知、规划、决策并执行任务。
   狭义的 Agent 则通常特指负责任务规划和工具选择的模块，核心由 LLM 提供推理和决策能力。
   换句话说，广义是“系统整体”，狭义是“决策调度组件”，两者层级不同，但都以 LLM 为核心智能来源。

问：大模型如何知道什么时候需要访问 MCP 工具，以及调用哪个工具？
答：在基本逻辑上，MCP 会提供工具的列表和使用说明，大模型根据用户请求推理选择需要的工具。但在实际系统中，为了提高效率和减少 token 消耗，通常会先通过意图识别或小模型筛选，再把必要的工具信息提供给大模型。

问：把所有工具信息放入模型上下文，会不会占用过多 token，影响理解能力？
答：会占用上下文窗口的 token，可能影响大模型的理解能力，但具体影响取决于模型大小、工具数量以及上下文管理策略。

问：如何优化工具调用流程，减少对大模型的压力？
答：常用做法是先用一个小模型进行意图识别，判断用户问题是否需要调用工具以及调用哪一个工具，只有必要的信息才会传给大模型，从而节省 token 并提高效率。

问：整体上怎么理解 MCP 工具与大模型的关系？
答：MCP 工具的选择和调用依赖大模型的推理能力，但现代系统会结合意图识别和代理层，兼顾工具丰富性和上下文效率。大模型负责理解和决策，Agent/中间层负责工具调用和结果管理。

问：MCP 是不是直接给 LLM 使用的协议？
答：不是。MCP 是 Agent 与工具之间的标准通信协议，负责协调工具调用和数据流。LLM 本身无法主动发起交互，它只负责生成文本。

问：LLM 是否可以直接使用 MCP 来调用工具？
答：不可以。LLM 只能根据上下文生成调用指令或请求，实际调用和执行由 Agent 负责。MCP 是 Agent 的接口，不是 LLM 的接口。

问：Function Calling 被 MCP 替代了吗？
答：没有。Function Calling 是 LLM 内部的结构化输出机制，用于生成调用指令。MCP 是外部协议，Agent 利用 MCP 调用工具。两者是不同层面的概念，Function Calling 与 MCP 可以协同使用。

问：为什么 LLM 不直接调用工具？
答：因为 LLM 是概率生成模型，没有主动执行外部操作的能力。通过 Agent 调用 MCP Server 的工具，LLM 只负责理解和推理，而 Agent 执行调用和管理结果，从而保证系统安全和可控。

Q：能否把 MCP Server 理解为一种“工具库微服务”？
A：可以这样理解，但要注意它并不等同于传统意义上的微服务。
MCP Server 更像是：
“通过统一协议暴露能力的工具端点”，
可以是本地脚本、插件、应用模块，也可以是网络服务。
它与微服务相似之处是：
都提供独立的功能
都可被其他系统调用
都可自由组合扩展
但 MCP 的重点是：
让所有工具以同一种方式被 LLM/Agent 使用，而不是分布式微服务架构。
“微服务（microservice）”更强调分布式部署、可水平扩展、跨网络通信等
MCP Server 可以只是：
本地脚本
一个 CLI
一个桌面应用的插件
或真的网络服务
→ 它不一定是微服务，只是一个“统一接口暴露的工具端点”。

我个人的疑问：

1. Agent里面是不是还有一个LLM？LLM不应该是Agent的大脑吗？Agent可以不包含LLM是么？在图片里面LLM和Agent是分开的吗？Agent到底是不是LLM再加上一些东西的集合呢，还是说在图片里Agent也包含着一个LLM，不过旁边也有一个LLM？
   **“Agent 是不是 = LLM？Agent 必须包含 LLM 吗？为什么有些图把它们画开？”**
   ✅ **核心先导结论（一句话版）**
   **Agent ≠ LLM。
   LLM 只是 Agent 的“大脑”，但 Agent 不是必须包含 LLM。
   Agent 是更大的系统范畴，而 LLM 是其中可能存在的一部分。**
   **1. LLM 是“推理大脑”，Agent 是“系统角色”**

- **LLM（大模型）**负责思考、推理、理解自然语言。
- **Agent**负责把“推理能力”放到一个系统中运作起来：
  - 接收任务
  - 规划步骤
  - 调用工具
  - 管理上下文
  - 处理错误
  - 维护长期状态（记忆、项目、任务树）
  - 与人交互
    👉 **Agent 是会“用一个大脑做事”的系统，而 LLM 就是其中的一块大脑。**
    📌 **2. Agent 里面是不是一定含 LLM？不一定。**
    很多情况下：
    ✔ **现代 AI Agent 一般都会包含一个 LLM**
    因为它需要推理能力、生成语言，并且做工具选择。
    ✖ **但从软件工程定义看，Agent 不一定需要 LLM**
    例子：
- 传统 Multi-Agent 系统（MAS）中，Agent 是一堆专家系统（无 LLM）
- 游戏 NPC 也是 Agent（基于规则系统）
- 智能推荐流程也可以被称为 Agent（无 LLM）
  **Agent 这个词比 LLM 年代更早，也更广义。**
  📌 **3. 那为什么很多图会把 Agent 和 LLM 画成两个方块？**
  因为你看到的图通常表达的是：
  > **“Agent 系统”调用“LLM 模块”，而不是反过来。**
  > 也就是说：
- Agent 是 orchestrator（编排器）
- LLM 是 reasoning engine（推理引擎）
  **就像浏览器（Agent）调用渲染引擎（LLM）一样。**
  📌 **4. 那 Agent = LLM + 一些东西，这个定义对吗？**
  答：**80% 的现代 Agent 确实是这样的 ——
  LLM + Memory + Tools + Workflow + Error Handling。**
  但这个定义不能覆盖全部情况，因为：
- 有些 Agent 并不包含 LLM（如传统智能体）
- LLM 也可以独立作为 Agent（如最简单的“纯对话 Agent”）
- Agent 可以包含多个 LLM（路由模型、工具模型、助手模型等）
  👉 **用“LLM + 一些周边能力”描述现代 Agent 是实用的，但不是理论定义。**
  📌 **5. 最标准、最通用的定义（建议你在文章中采用）**
  **Agent（智能体）＝
  能感知环境、做出决策并采取行动的系统。**
- 感知（输入）
- 决策（推理）
- 行动（执行）
  LLM 只是 **“强决策能力的模块”**。
  现代 AI Agent 通常是：
  \*\*LLM（推理）
- 工具（行动）
- 工作流（规划）
- 状态管理（记忆、上下文）
- 接口（看世界、影响世界）\*\*
  所以：
  > **Agent「可以」包含 LLM，但不是所有 Agent 都必须包含。
  > LLM「可以」独立作为一个弱 Agent，但通常不够强。**
  > 📌 **一句极简版总结（你可以放博客里）** > **LLM 是 Agent 的“大脑”，但 Agent 是比 LLM 更大的系统。
  > Agent 不一定包含 LLM，而现代 AGI 风格的 Agent 绝大多数都包含。
  > 在技术图里把 Agent 和 LLM 分开，是为了强调：LLM是能力，Agent是系统角色。**

2. 在图片中的这八个步骤中，一共有几个步骤是会消耗token的？
   ![示例图片](assets/ps.png)
   直接说结论：只有与 LLM 交互的步骤会产生“token”开销（即把文本/结构化字段发送给模型或从模型返回文本/结构化输出）。
   与 MCP Server /工具的网络请求本身不消耗 LLM 的 token（但这些工具可能有自己的网络/API费用）。另外要注意：有些厂商把 tool schema 当作独立字段传输，不计入 prompt token，具体细节依厂商 API 而异。
   下面给你按步骤逐项判断（简洁、明确）：
   用户提问（“我女朋友肚子疼…”）
   → 不直接消耗 LLM token（直到 Agent 把它发送给模型为止）。
   Agent 从 MCP Server 获取工具信息
   → 不消耗 LLM token（只是内部/网络调用到 MCP）。
   Agent 把 tool 信息 + user prompt 封装并发送给模型（system 或 function-calling）
   → 会消耗 token（输入）。如果 tool schema是以普通文本塞进 system prompt，会占用大量 token；若通过 API 的结构化字段传入，则可能不占 prompt token（视 API 而定）。
   模型产生“调用 web_browse 的请求”（普通回复或 function_call）
   → 会消耗 token（输出）——这是模型返回的内容（无论普通文本还是结构化 function call 都有输出 token）。
   Agent 用 MCP 调用 web_browse 工具（爬网页）
   → 不消耗 LLM token（是工具/网络调用；但工具可能有自己的费用）。
   Agent 把 web_browse 的结果（网页内容/摘要）再发回给模型
   → 会消耗 token（输入）——把工具结果作为新一轮模型输入；若做了压缩/摘要或只传片段，能节省 token。
   模型基于网页内容生成最终答案（“多喝热水”等）
   → 会消耗 token（输出）——模型返回最终文本。
   Agent 将结果展示给用户
   → 不消耗 LLM token（只是展示/前端渲染）。
   一句话结论（最常见计费场景）
   通常会有 两次与 LLM 的往返 导致 token 消耗：
   第一次：Agent → LLM（输入） + LLM → Agent（输出：tool call）
   第二次：Agent → LLM（输入：tool 结果） + LLM → Agent（输出：最终答案）
   因此实务中你会为 两轮请求的输入和输出 token 付费（中间的 MCP/工具调用不计入 LLM token，但可能有其它成本）。
   额外注意（工程细节）
   如果系统使用意图识别/向量路由，把工具筛选放到模型外，会减少发送给主模型的 token。
   如果厂商把 tool schema 作独立参数传输（非 prompt 文本），则首次发送的 token 会更少。
   任何对网页结果的全文传回都会非常消耗 token，常用做法是先摘要/抽取再送回模型以节省成本。

3. system prompt是什么时候传给LLM的？system prompt有没有消耗token？实在每一次对话的时候都会传一遍吗？system prompt里面会具体包含什么东西？
   Q1：System prompt 是什么时候传给 LLM 的？
   A：**每一次向 LLM 发起请求时都会一起发送。**LLM 是无状态的，它不会记住之前的 system prompt，所以每轮都必须重新带上。
   Q2：System prompt 会消耗 token 吗？
   A：**会。**system prompt 完整计入输入 token，和 user prompt 的计费方式完全一致。
   Q3：多轮对话时，需要每次都重新发送 system prompt 吗？
   A：**是的。**多轮对话 API 是无状态的，Agent 必须自己维护对话历史并在每次请求里带上 system prompt + 历史内容。
   Q4：System prompt 里通常会放什么？
   A：一般包含 3 类内容：
   角色 & 行为规则（例如：你是一个医生，请简洁回答）
   输出格式要求（例如：永远用 JSON）
   工具使用方法（如果没有 function calling API，需要在 system prompt 里描述如何触发工具）

   ```json
   {
     "model": "gpt-4.1",
     "messages": [
       {
         "role": "system",
         "content": [
           "你是一个具备工具调用能力的 AI 助理。",
           "当你需要使用工具时，请务必返回严格的 JSON：",
           "",
           "{ \"tool\": \"TOOL_NAME\", \"args\": { ... } }",
           "",
           "可用工具如下：",
           "1. search(keyword): 发送搜索请求",
           "2. weather(city): 返回天气信息",
           "",
           "如果不需要工具，就直接正常回答用户。"
         ].join("\n")
       },
       {
         "role": "user",
         "content": "帮我查一下北京天气"
       }
     ]
   }
   ```

   ⭐ 一句话总结
   System prompt 是 LLM 的“本轮配置文件”：每次请求都会发送、会计费、必须重复传递，用来告诉模型“你是谁、你怎么答、格式是什么、工具咋用”。

4. 为什么现在人人都在讨论 MCP？因为所有人意识到：真正的 AI 革命，不是更聪明的模型，而是“模型能做更多事情”。而让模型“做事情”的关键就是：
   能力接入标准化 → 工具复用 → 不同模型之间共享能力。

---
title: 重新理解 AI Agent：它与 LLM 不是替代关系，而是进化
description: '我讲从示例、原理到工程架构全面解析 Agent 的感知、记忆、工具与推理能力，带你重新理解“智能体”的真正边界。'
pubDate: 2025-11-23
cover: assets/cover.png
tags: ['AI', 'Agent', 'LLM']
recommend: true
---

### 💡 1. 先看一个例子

> **帮我订一个明天飞上海的机票，预算 500 元以内，并把可选方案发给我。**

**🧠 LLM 会怎么做？**

根据已训练的数据“猜测机票价格”给你几段建议，但不会去查真实航班，更不会帮你下单

示例输出：

> 明天去上海的机票大约在 X~Y 元之间，建议去携程、飞猪搜索。

👉 本质上是在**说**，不是在**做**。

**🤖 Agent 会怎么做？**

1. **识别用户意图**
2. 调用真实 API 查询航班
3. 过滤预算范围
4. 生成可点击的可购买方案
5. 如果你授权，还能直接帮你下单

示例输出：

> 为你找到 3 个符合预算的航班：
>
> - 09: 20 — ¥468
> - 12: 55 — ¥495
> - 15: 40 — ¥499  
>   点击即可购买。

👉 **Agent 不是回复你，而是完成任务。**

---

### 🤖 2. 所以什么是 LLM？什么是 Agent？

🧠 **LLM：语言与推理引擎（会说，不会做）**

LLM 能生成文本、推理问题、理解意图、输出结构化内容（如 JSON / Schema）、甚至生成 SQL/代码；
但它不能执行真实动作（写文件、下单、API 调用）、不能与系统交互、不能保存长期记忆、不能控制状态机，也无法稳定完成多轮复杂任务。

- LLM = 一个超级语言补全器 + 世界模型

🤖 **Agent：让 LLM 变成“能行动的智能体”**

Agent 必须具备完整闭环：感知 → 记忆 → 推理 → 工具调用 → 执行动作 → 观察结果 → 再推理 → 完成任务

- Agent = LLM（大脑） + 工具（手脚） + 记忆系统 + 执行器 + 控制循环 + 安全边界

**🔥 那么为什么 Agent 不是“带工具的 LLM”？**

因为 Agent 解决的问题比 LLM 多得多。因为 Agent 背后有大量 LLM 之外的能力：
⚠️下面表中记录的只是暂时的，如果基础大模型进化也可能会改变表格里的内容

| 类别         | LLM               | Agent |
| ------------ | ----------------- | ----- |
| 推理         | ✔️                | ✔️    |
| 工具调用     | ❌                | ✔️    |
| 状态管理     | ❌                | ✔️    |
| 长期记忆     | ❌                | ✔️    |
| 对外行动     | ❌                | ✔️    |
| 自我修正     | ❌                | ✔️    |
| 多轮任务执行 | ❌                | ✔️    |
| 目标规划     | ❌（需要 prompt） | ✔️    |
| 安全控制     | ❌                | ✔️    |

一句话总结：

> **LLM 只能“说”，Agent 才能“做”。**

---

### 🧱 3. Agent 的四大核心组件

Agent 若要能“持续行动”，必须至少具备四类能力（感知、记忆、推理、工具）。
![示例图片](assets/mindmap.png)

那怎么个持续运动呢？请看下面的Agent工作循环⬇️

### ♻️ 4. Agent 的标准工作循环

下面是标准的 Agent 循环伪代码：

```python
while not done:
    # 1. 读取 “状态”
    context = memory.retrieve()

    # 2. 基于状态推理下一步
    thought = LLM.plan(context)

    # 3. 判断模型的意图是不是 “要执行工具”
    action = parse(thought)

    # 4. 执行工具
    result = tools.execute(action)

    # 5. 将观察写入记忆（外部长期状态）
    memory.write(action, result)

    # 6. 判断任务是否达成
    done = check_goal(result)
```

**翻译成中文也就是这五件事：**

| 步骤       | Agent 必要能力 | 作用                         |
| ---------- | -------------- | ---------------------------- |
| 读取记忆   | 状态管理       | 让模型知道“我上一轮做到哪了” |
| 推理下一步 | 规划           | 决定下一步动作               |
| 执行工具   | 行动           | 让“语言输出”变成“真实动作”   |
| 写入记忆   | 状态更新       | 保证下一轮不会遗忘           |
| 判断完成   | 收敛机制       | 避免无限循环                 |

**🧩 我们可以用一个例子理解这个步骤：自动化报销 Agent**

假设要做一个公司内部的“报销助手 Agent”，它能：

1. 解析用户上传的发票
2. 自动分类
3. 查预算
4. 创建报销单
5. 提交审批

它的每一步都严格按上面循环：

**第一轮**

- 读记忆：空
- 规划：用户上传了发票，我先 OCR
- 行动：调用 `read_invoice()`
- 写记忆：发票金额 = 320, 类别 = 交通

**第二轮**

- 读记忆：发票金额、类别
- 规划：需要查预算剩余额度
- 行动：`get_budget(user_id)`

**第三轮**

- 读记忆：预算 = 500
- 规划：可以创建报销单
- 行动：`create_ticket(320, "交通")`

**第四轮**

- 读记忆：报销单编号
- 规划：提交审批
- 行动：`submit_approval(ticket_id)`
- 达成任务 → done

你会发现：

> **一个 Agent 不是“一次调用模型”，而是一套“带状态的推理—行动循环系统”。**
> LLM 不是核心，**循环 + 状态 才是核心。**

---

### 🚌 5.Agent的分类

当前工业界普遍采用三类 Agent 范式。它们并不是互斥关系，而是依据任务复杂度、成本、推理深度来做取舍。下面对三类范式进行工程化拆解，并补充各自的优势、限制与典型应用。

**三大 Agent 设计范式对照表**

| 属性                 | **Reactive（反应式）**                   | **Deliberative（规划式）**                                 | **ReAct（推理+行动）**                                         |
| -------------------- | ---------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- |
| **思维方式**         | 无规划、即时反应                         | 先思考再行动、有显式 Plan                                  | 边做边想（Thought → Action → Observation）                     |
| **主要特征**         | 不拆分任务；无多步链；输入→输出；延迟低  | 生成明确 Plan；任务拆解；逐步验证；推理深但成本高          | 有状态循环；支持自我纠错；在能力与成本间折中                   |
| **适用场景**         | 助手问答、单步命令                       | 代码调试、法律文书、多文档推理、企业自动化                 | 复杂多步骤任务、跨工具工作流、动态环境                         |
| **典型示例**         | 生成邮件草稿；总结单文档；普通聊天机器人 | GPT-o1、Anthropic Slow Thinking；自动写 PRD→生成测试→跑 CI | 旅行规划（查天气→查酒店→预订）；客服工单自动化；数据分析 Agent |
| **优缺点（一句话）** | 成本低、延迟小，但能力受限，非完整 Agent | 推理能力强、可靠但昂贵，适合高风险任务                     | 平衡速度与能力、工程化首选，适合大多数生产场景                 |

**📚 补充：其他常见 Agent 分类方式**

除了主流的“三分法”，工程实践中还有其他几种分类视角。

**🔧 按环境交互方式分类**

| 类型         | 特征               | 应用            |
| ------------ | ------------------ | --------------- |
| 单轮 Agent   | 一次输入，一次输出 | 助手问答        |
| 多轮 Agent   | 有循环、有上下文   | 工作流自动化    |
| 交互式 Agent | 持续与环境互动     | 游戏 AI、机器人 |

**🔧 按控制方式分类**

| 类型                 | 谁主导流程？           | 示例              |
| -------------------- | ---------------------- | ----------------- |
| LLM-Controlled Agent | 一切由大模型决定       | ReAct、AutoGPT    |
| Orchestrated Agent   | 程序做编排，LLM 做判断 | LangGraph         |
| Hybrid Agent         | 流程图 + 推理混合控制  | Devin、企业工作流 |

**🔧 按 Agent 数量分类**

| 类型                | 特点           | 应用              |
| ------------------- | -------------- | ----------------- |
| 单 Agent            | 简单、稳定     | 办公助理          |
| 多 Agent            | 分工协作       | 客服、开发自动化  |
| 群体 Agent（Swarm） | 类社会群体行为 | 游戏 AI、金融仿真 |

---

### ❗ 6. Agent 的工程难点（真实开发会遇到的）

现实中做 Agent，远比“加个工具调用”难得多：

- 上下文窗口有限 → 需要分段检索、摘要
- 工具调用失败 → 需要回滚、重试逻辑
- 幻觉问题 → 需要验证器（verifier）
- 成本与延迟 → LLM 调用昂贵
- 长期记忆管理困难
- 权限管理与安全风险

所有成熟 Agent 框架（LangChain、Meta Agent、OpenAI Agent API）都在解决这些问题。
这就是为什么成熟的 Agent 框架变得如此重要。

---

### 📊 7. 如何评价一个 Agent？

- **任务成功率**
- **工具调用次数**
- **延迟 / 调用成本**
- **错误恢复能力**
- **安全性与权限合规**
- **可解释性（日志链路）**

---

### 🎯 8. 总结：Agent 的本质

AI Agent 不是“更智能的聊天机器人”，也不是“带工具的 LLM”，而是一个：
能感知、能规划、能行动、能调用工具、能观察、能自我纠错、能持续多轮执行的**智能行动系统**。
随着工具生态、长期记忆技术、验证器、Agent 框架的发展，Agent 很可能成为软件开发的下一代范式：
**从写代码 → 到写意图 从编程 → 到引导**

> **LLM 是大脑
> Agent 是生命体
> 而未来的软件，就是无数 Agent 的协作网络。**

---
